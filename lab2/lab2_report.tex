\documentclass[10pt]{article}
\usepackage{../setup}
\vspace{-8ex}
\date{}

\graphicspath{ {./figs/} }

\begin{document}

\title{\textbf{\Large{\textsc{ECE410:} Linear Control Systems}} \\ \Large{Lab 2 Report: Numerical Linear Algebra and Controllability} \\ \textbf{\small{PRA102}}\vspace{-0.3cm}}
\author{Pranshu Malik, Varun Sampat \\ \footnotesize{1004138916}, \footnotesize{1003859602}\vspace{-3cm}}

\maketitle

\section{Numerical Linear Algebra}
\subsection{Basic operations on subspaces}
\subsubsection{Output 1}
Recall $ \mathcal{V} $ and $ \mathcal{W} $ were subspaces defined as:

\begin{equation*}
    \mathcal{V} = \text{span}
    \begin{Bmatrix}
        \rcvec{1\\-1\\0\\1},
        \rcvec{1\\1\\0\\0},
        \rcvec{3\\1\\0\\1}
    \end{Bmatrix}
    \quad
    \text{and}
    \quad
    \mathcal{W} = \text{span}
    \begin{Bmatrix}
        \rcvec{1\\0\\2\\1},
        \rcvec{1\\0\\-2\\0}
    \end{Bmatrix}
\end{equation*}

For these subspaces, \texttt{orth} returned:
\begin{equation*}
    \mathscr{Basis}_{\mathcal{V}} = \text{span}
    \begin{Bmatrix}
        \rcvec{-0.9089\\-0.2650\\0\\-0.3220},
        \rcvec{-0.0849\\0.8736\\0\\-0.4792}
    \end{Bmatrix}
    \quad
    \text{and}
    \quad
    \mathscr{Basis}_{\mathcal{W}} = \text{span}
    \begin{Bmatrix}
        \rcvec{-0.0399\\0\\-0.9645\\-0.2611},
        \rcvec{-0.8988\\0\\0.1488\\-0.4122}
    \end{Bmatrix}
\end{equation*}


The column space in $\mathcal{V}$ is not linearly independent. The rank$(\mathcal{V})$ is 2 < the dim($\mathcal{V}) = 3$. Note that the dim($\mathcal{V}$) would be the number of vectors in $\mathcal{V}$. This leads to the orthonormal basis of $\mathcal{V}$ containing only 2 vectors instead of 3. The column space in $\mathcal{W}$ was linearly independent. The orthonormal basis contains the same number of column vectors as $\mathcal{W}$ itself. One interesting point to note is that $\mathcal{W}$ contained $\rcvec{1 & 0 & -2 & 0}^\intercal$, but in its orthonormal basis, the 0 in the last row did not show up. This is because the \textsc{MATLAB} function \texttt{orth} returns vectors that are mutually orthogonal (hence linearly independent) and normalized. In $\mathscr{Basis}_{\mathcal{W}}$, one vector has a component along $\vec{e}_4$ and since it is not entirely orthogonal to the other vector, upon orthonormalization, we see that both vectors help span a hyperplane, each with support over $\vec{e}_4$.

\subsubsection{Output 2}
For the same $\mathcal{V}$ and $\mathcal{W}$ defined in section 1:
\begin{equation*}
    \text{Im}({\mathcal{V} + \mathcal{W}}) = \text{span}
    \begin{Bmatrix}
        \rcvec{-0.9004\\-0.2119\\-0.0950\\-0.3680},
        \rcvec{0.1634\\0.0904\\-0.9611\\-0.2038},
        \rcvec{0.0513\\-0.8871\\-0.1654\\0.4279}
    \end{Bmatrix}
\end{equation*}
\begin{center}
    and
\end{center}
\begin{equation*}
    \text{Im}({\mathcal{V} \cap \mathcal{W}}) = \text{span}
    \begin{Bmatrix}
        \rcvec{-0.8944\\0\\0\\-0.4472}
    \end{Bmatrix}
\end{equation*}

To ensure these computations are correct, we could test if ${\mathcal{V} \cap \mathcal{W}} \in \text{Im}({\mathcal{V} + \mathcal{W}})$. This is because any vector that is contained in both the vector spaces should also be contained in the vector space defined as the sum of the two vector spaces. Essentially, we want to test if all vectors spanned by ${\mathcal{V} \cap \mathcal{W}}$ can be expressed as a linear combination of $\mathcal{V} + \mathcal{W}$:
\begin{equation*}
    \text{rank}(\mathcal{V} + \mathcal{W}) = 3
    \quad
    \text{and}
    \quad
    \text{rank}\left(\rcvec{\mathcal{V} + \mathcal{W} & \mathcal{V} \cap \mathcal{W}}\right) = 3
\end{equation*}

The addition of $\mathcal{V} \cap \mathcal{W}$ does not affect the $\text{Im}$ of $\mathcal{V} + \mathcal{W}$. Therefore, we can conclude that these computations are correct.

\subsection{Linear transformations and linear equations}
\subsubsection{Output 3}
Consider the basis $\{\vec{x}_1, \vec{x}_2\}$ of $\mathbb{R}^2$, where:
\begin{equation*}
    \vec{x}_1 = \rcvec{1\\1}
    \quad
    \text{and}
    \quad
    \vec{x}_2 = \rcvec{2\\1}
\end{equation*}

The change of basis (hence invertible) matrix $P$ will be defined as the concatenation of $\vec{x}_1$ and $\vec{x}_2$:
\begin{equation*}
    P =
        \begin{bmatrix}
            \vec{x}_1 & \vec{x}_2
        \end{bmatrix} = 
        \begin{bmatrix}
            1 & 2\\1 & 1
        \end{bmatrix}
\end{equation*}
Recall, the change of basis relationship ($\vec{x} \mapsto \vec{z} $)  is defined as the following mathematical relationship:

\begin{equation} \label{change_basis}
    \vec{z} = P^{-1} \cdot \vec{x}
\end{equation}

Applying this formula for the given $\vec{x} = \rcvec{2 & 1}^\intercal$ gives us $\vec{z} = \rcvec{z_1 & z_2} = \rcvec{0 & 1}^\intercal$

To verify this answer, (\ref{change_basis}) can be rearranged to return to the original basis, i.e., $\vec{z} \mapsto \vec{x} $:
\begin{equation*}
    \vec{x} = z_1\rcvec{1\\1} + z_2\rcvec{2\\1} = 0\rcvec{1\\1} + 1\rcvec{2\\1} = \rcvec{2\\1}
\end{equation*}

Hence, we numerically reconstructed $\vec{x}$ and verified that the change of basis computed the correct $\vec{z}$.

\subsubsection{Output 4}
A linear transformation $\vec{A} : \mathbb{R}^n \rightarrow \mathbb{R}^m$ given by $\vec{y} = \vec{A}(\vec{x}) = A\vec{x}$ can be expressed in the general coordinate frames, different from the standard bases, $P = \rcvec{\vec{x}_1 & \cdots & \vec{x}_n} \in \mathbb{R}^{n\times n}$ and $Q = \rcvec{\vec{y}_1 & \cdots & \vec{y}_m} \in \mathbb{R}^{m\times m}$, whose columns constitute bases for $\mathbb{R}^n$ and $\mathbb{R}^m$ respectively. Let this transformation between the two non-standard bases be given by,
\[
    \vec{w}=\hat{A}\vec{z},
\]
where, $\vec{x}=P\vec{z}$ and $\vec{y} = Q\vec{w}$. Then, we can write:
\begin{align*}
    \vec{w} = Q^{-1}\vec{y} &= \hat{A}P^{-1}\vec{x} = \hat{A}\vec{z}\\
    \implies \quad \vec{y} &= Q\hat{A}P^{-1}\vec{x} = A\vec{x}
\end{align*}

Therefore, we have that $\hat{A} = Q^{-1}AP=\texttt{Q\textbackslash (AP)}$ in \textsc{MATLAB} code, which means that vector ${\hat{A}}[:, j]$ is the coordinate of vector $AP[:, j]$ in basis $Q$ where $AP[:, j]$ is the mapping of the $j^{\text{th}}$ basis vector in $P$ into $\mathbb{R}^m$, i.e. $AP$ maps $P$-coordinates into $\mathbb{R}^m$ in the standard basis and $Q^{-1}$ further maps these standard coordinates into the non-standard basis.

\subsubsection{Output 5}
Given a matrix $A$, to test for its injectivity, we simply need to see if the $\text{dim}(\text{Ker}A) = 0$. If it is nonzero, then the zero vector, $\vec{0}$, and at least one non-zero vector (in the nullspace) will have outputs equal to $\vec{0}$, implying that the linear transformation $A\vec{x}$ is not injective. This is only true if $A$ has full column rank. Similarly, as a test for surjectivity, we need the $\text{rank}(A)$ to equal the dimension of the co-domain, i.e., have full row rank. If $A$ is not a full rank matrix, then it would neither be injective nor surjective, and on the other hand, if $A$ is a full-rank square matrix, then it is a bijective map.

The rank($\vec{A}$) was obtained using the \texttt{rank} function, and it was determined to be $3$. With the number of columns in $\vec{A}$ being $4$,the rank-nullity theorem can be applied to determine the nullity($\vec{A}$):
\begin{equation*}
    \text{nullity}(\vec{A}) = 4 - \text{rank}(\vec{A}) = \texttt{size(A,2) - rank(A)}= 4 - 3 = 1
\end{equation*}

This can be verified by directly computing the dimension of the nullspace of $\vec{A}$:
\begin{equation*}
    \text{nullity}(\vec{A}) = \text{dim(nullspace}(\vec{A})) = \texttt{size(null(A),2)} = 1
\end{equation*}

And by the programmatic implementation of the matrix map characterization criteria detailed above, it was determined that $A$ is neither an injective nor a surjective map.

\subsubsection{Output 6}
\textcolor{red}{Add some intro theory and add MATLAB results â€“please format the table!}

The rank($\vec{A}$) = 3. 

By checking rank($[\vec{A} | \vec{b}]$), where $\vec{b} \in \mathbb{R}^n$, we are essentially checking if $\vec{b}$ is a linear combination of the column space of $\vec{A}$. This logic was exploited in \texttt{Output 2 (1.1.2)}. 

Note, if the rank is affected (i.e., if it increases), then there does not exist an $\vec{x}$ that solves $\vec{A}\vec{x} = \vec{b_i}$. If the rank is unaffected, then there is at least one solution. If the nullity($\vec{A}$) is 0, then the solution will be unique. If not, there will be an infinite number of solutions to the system.  

\textcolor{red}{Add some injectivity and surjectity knowledge.}

The following table shows the rank($[\vec{A} | \vec{b_i}]$), for each of the input $\vec{b}$ vectors provided:

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c|c}
        $b_i$ & rank($[A|b_i]$) & Number of solutions & Solution & comments \\ 
        $\rcvec{1&0&0&0&0}^T$ & 4 & 0 & N/A & The rank increases, implying no solution\\
        $\rcvec{1&1&-2&-2&-2}$ & 3 & $\infty$ & $\rcvec{2 & -1 & 0 & -1}^T$ & The nullity(A) is 1, so there are an infinite number of solutions. It is easy to generate another solution to the system by taking $x' = x + ky$, where x is the solution determined by using $\texttt{A\textbackslash b2}$, y is any vector in the nullspace and $k \in \mathbb{R}$
    \end{tabular}
    \caption{Caption}
    \label{tab:my_label}
\end{table}

\section{A-invariance and Representation Theorem}
A subspace $\mathcal{V}$ is $A$-invariant if and only if
\[
    A\mathcal{V} \subset \mathcal{V},
\]
i.e., for any vector $\vec{v} \in \mathcal{V}$ we have that $A\vec{v} \in \mathcal{V}$.
\subsection{Testing for A-invariance}
We can test this by...
\textcolor{red}{Add some intro theory and add MATLAB results}



% #################################### Output 7 ####################################
\subsubsection{Output 7}
\textcolor{red}{Add some intro theory and add MATLAB results}

It is sufficient to check $\texttt{rank}(V) == \texttt{rank}([AV | V])$ to check numerically if $\mathcal{V}$ is $A$-invariant in $\vec{A}$. rank($V$) = 2 = rank($[AV | V]$) and hence $\mathcal{V}$ is $A$-invariant.

To find $P = [V | W]$, the next steps only include computing $W$, which is the independent complement of $V$, it is sufficient enough to find a basis for the kernel of $V^T$:
\begin{equation*}
    W = \mathscr{Basis}_{Ker(\mathcal{V^T})} = \texttt{null(V')} = \rcvec{0.5774 & 0.5774 & 0.5774}^T
\end{equation*}

With $V$ and $W$, $P = [V | W]$ can be constructed, allowing us to compute $P^{-1}AP$:
\begin{equation*}
    P^{-1}AP = \texttt{P\textbackslash(A*P)} = \begin{bmatrix}
        2 & 0 & 0 \\
        0 & -1 & 2.3094 \\ 
        0 & 0 & 1
    \end{bmatrix}
\end{equation*}

It can be visually verified that $P^{-1}AP$ is block upper-triangular.

% ################################## Output 8 ######################################
\section{Controllability and Kalman Decomposition}
What is controllability and Kalman: 2 sentences each with all terms needed.
\textcolor{red}{Add some intro theory and add MATLAB results}
\subsection{Controllability}
For a linear state-space system with matrices $(\vec{A},\vec{B})$
\[
    Q_c \coloneqq \rcvec{B & AB & \cdots & A^{n-1}B}
\]

For the given $\vec{A}$ and $\vec{B}$, the controllability matrix $Q_c$ was computed by using the \texttt{ctrb} function in \texttt{MATLAB}:
\begin{equation*}
    \begin{bmatrix}
        1 & -2 & 10 & 0 & 20 & 40 \\
        0 & 0 & 0 & 0 & 0 & 0 \\
        1 & 2 & -6 & 8 & -44 & -8
    \end{bmatrix}
\end{equation*}

\subsection{Kalman Decomposition for Controllability}
\subsubsection{Output 8}
To determine $\hat{A}$ and $\hat{B}$, $\mathcal{V}$ must be determined and then the same steps used in \texttt{Output 7} can be utilized. $\mathcal{V}$ is defined as basis(Im($Q_c$)) and hence an $A$-invariant subspace. Using the \texttt{orth} function can applied to compute $\mathcal{V}$, which results in:
\begin{equation*}
    \mathcal{V} = \begin{bmatrix}
        -0.7071 & -0.7071 \\
        0 & 0 \\ 
        0.7071 & -0.7071
    \end{bmatrix}
\end{equation*}

$\mathcal{W}$ can be computed similar to how it was in \texttt{section 2}, i.e., $\mathcal{W} = \text{null}(\mathcal{V}^T)$. $\mathcal{W}$ was computed to be:
\begin{equation*}
    \mathcal{W} = 
        \begin{bmatrix}
            0\\1\\0
        \end{bmatrix}
\end{equation*}

Finally, $P = [\mathcal{V} \text{ | } \mathcal{W}]$, which is concatenating the computed $\mathcal{V}$ and $\mathcal{W}$. With that $P$, the Kalman decomposition for this system ($\hat{A}$ and $\hat{B}$) can be determined:
\begin{equation*}
    \hat{A} = P^{-1}\vec{A}P = \texttt{P\textbackslash(A*P)} = 
    \begin{bmatrix}
        2 & 8 & 11.3137 \\
        -2 & 2 & 4.2426 \\ 
        0 & 0 & -6
    \end{bmatrix}
\end{equation*}

Similarly, $\hat{B}$ can be computed as:
\begin{equation*}
    \hat{B} = P^{-1}B = \texttt{P\textbackslash B} = 
    \begin{bmatrix}
        0 & 2.8284 \\
        -1.4142 & 0 \\ 
        0 & 0 
    \end{bmatrix}
\end{equation*}

Our system can be expressed as:
\begin{equation*}
    \begin{bmatrix}
        \dot{z_1} \\
        \dot{z_2} \\
        \dot{z_3}
    \end{bmatrix}
    = 
    \hat{A} \cdot \begin{bmatrix}
        z_1\\z_2\\z_3
    \end{bmatrix} +
    \hat{B} \cdot \begin{bmatrix}
        u_1 \\ u_2
    \end{bmatrix}
\end{equation*}

To split the system into its controllable and uncontrollable subsystems, we can use the structure of the $\hat{A}$ matrix:
\begin{equation*}
    \hat{A} = 
    \begin{bmatrix}
        A_{11} & A_{12}\\
        0 & A_{22}
    \end{bmatrix}
\end{equation*}
Where $A_{11} \in \mathbb{R}^{k\times k}$, $A_{12} \in \mathbb{R}^{k \times n-k}$, $\vec{0} \in \mathbb{R}^{n-k \times k}$, $A_{22} \in \mathbb{R}^{n-k \times n-k}$, $k$ is the rank($Q_c$) (or the number of column vectors in $\mathcal{V}$) and n is the number of rows in $Q_c$.

Using $k = 2$ and $n = 3$, the controlloable system can be defined as:
\begin{equation*}
    \dot{z_1} = 2z_1 + 8z_2 + 11.3137z_3 + 2.8284u_2
\end{equation*}
\begin{equation*}
    \dot{z_2} = - 2z_1 + 2z_2 + 4.2426z_3 - 1.4142u_1
\end{equation*}

And the uncontrollable system:
\begin{equation*}
    \dot{z_3} = -6z_3
\end{equation*}

Note, while $z_3$ cannot be controlled directly or indirectly, it doesn't blow up. 

\end{document}