\documentclass[10pt]{article}
\usepackage{../setup}
\vspace{-8ex}
\date{}

\graphicspath{ {./figs/} }

\begin{document}

\title{\textbf{\Large{\textsc{ECE410:} Linear Control Systems}} \\ \Large{Lab 2 Report: Numerical Linear Algebra and Controllability} \\ \textbf{\small{PRA102}}\vspace{-0.3cm}}
\author{Pranshu Malik, Varun Sampat \\ \footnotesize{1004138916}, \footnotesize{1003859602}\vspace{-3cm}}

\maketitle

\section{Numerical Linear Algebra}
\subsection{Basic operations on subspaces}
\subsubsection{Output 1}
Recall $ \mathcal{V} $ and $ \mathcal{W} $ are subspaces defined as:

\begin{equation*}
    \mathcal{V} = \text{span}
    \begin{Bmatrix}
        \rcvec{1\\-1\\0\\1},
        \rcvec{1\\1\\0\\0},
        \rcvec{3\\1\\0\\1}
    \end{Bmatrix}
    \quad
    \text{and}
    \quad
    \mathcal{W} = \text{span}
    \begin{Bmatrix}
        \rcvec{1\\0\\2\\1},
        \rcvec{1\\0\\-2\\0}
    \end{Bmatrix}
\end{equation*}

For these subspaces, \texttt{orth} returned:
\begin{equation*}
    \mathscr{Basis}_{\mathcal{V}} = \text{span}
    \begin{Bmatrix}
        \rcvec{-0.9089\\-0.2650\\0\\-0.3220},
        \rcvec{-0.0849\\0.8736\\0\\-0.4792}
    \end{Bmatrix}
    \quad
    \text{and}
    \quad
    \mathscr{Basis}_{\mathcal{W}} = \text{span}
    \begin{Bmatrix}
        \rcvec{-0.0399\\0\\-0.9645\\-0.2611},
        \rcvec{-0.8988\\0\\0.1488\\-0.4122}
    \end{Bmatrix}
\end{equation*}

Taking the subspace spanning vectors as columns in matrices $V$ and $W$ for subspaces $\mathcal{V}$ and $\mathcal{W}$ respectively, we make the following observations. The column vectors in $V$ are not linearly independent as $\text{rank}(V) = 2$ which is one less than the number of columns in $V$. Note that the $\text{rank}(V)$ would be the number of basis vectors in $\mathcal{V}$. This leads to the orthonormal basis of $\mathcal{V}$ containing only 2 vectors instead of 3. On the other hand, the column vectors in $W$ was linearly independent. The orthonormal basis contains the same number of column vectors as $W$ itself. One interesting point to note is that $W$ contains $\rcvec{1 & 0 & -2 & 0}^\intercal$, but in its orthonormal basis, the $0$ in the last row did not show up. This is because the \textsc{MATLAB} function \texttt{orth} returns vectors that are mutually orthogonal (hence linearly independent) and normalized. In $\mathscr{Basis}_{\mathcal{W}}$, one vector has a component along $\vec{e}_4$ and since it is not entirely orthogonal to the other vector, upon orthonormalization, we see that both vectors help span a hyperplane, each with support over $\vec{e}_4$.

\subsubsection{Output 2}
For the same $\mathcal{V}$ and $\mathcal{W}$ defined in section 1:
\begin{equation*}
    {\mathcal{V} + \mathcal{W}} = \text{span}
    \begin{Bmatrix}
        \rcvec{-0.9004\\-0.2119\\-0.0950\\-0.3680},
        \rcvec{0.1634\\0.0904\\-0.9611\\-0.2038},
        \rcvec{0.0513\\-0.8871\\-0.1654\\0.4279}
    \end{Bmatrix}
\end{equation*}
\begin{equation*}
    {\mathcal{V} \cap \mathcal{W}} = \text{span}
    \begin{Bmatrix}
        \rcvec{-0.8944\\0\\0\\-0.4472}
    \end{Bmatrix}
\end{equation*}

To ensure these computations are correct, we could test if ${\mathcal{V} \cap \mathcal{W}} \subset \mathcal{V} + \mathcal{W}$. This is because any vector that is contained in the intersection should also be contained in the sum of the two vector spaces. Essentially, we want to test if all vectors spanning ${\mathcal{V} \cap \mathcal{W}}$ can be expressed as a linear combination of vectors in $\mathcal{V} + \mathcal{W}$:
\begin{equation*}
    \text{dim}(\mathcal{V} + \mathcal{W}) = 3
    \quad
    \text{and}
    \quad
    \text{dim}\left((\mathcal{V} + \mathcal{W}) + (\mathcal{V} \cap \mathcal{W})\right) = 3
\end{equation*}

The addition of $\mathcal{V} \cap \mathcal{W}$ does not affect the dimension of $\mathcal{V} + \mathcal{W}$. Therefore, we have increased the confidence towards correctness of our computations.

\subsection{Linear transformations and linear equations}
\subsubsection{Output 3}
Consider the basis $\{\vec{x}_1, \vec{x}_2\}$ of $\mathbb{R}^2$, where:
\begin{equation*}
    \vec{x}_1 = \rcvec{1\\1}
    \quad
    \text{and}
    \quad
    \vec{x}_2 = \rcvec{2\\1}
\end{equation*}

The change of basis (hence invertible) matrix $P$ will be defined as the concatenation of $\vec{x}_1$ and $\vec{x}_2$:
\begin{equation*}
    P =
        \begin{bmatrix}
            \vec{x}_1 & \vec{x}_2
        \end{bmatrix} = 
        \begin{bmatrix}
            1 & 2\\1 & 1
        \end{bmatrix}
\end{equation*}
Recall, the change of basis relationship ($\vec{x} \mapsto \vec{z} $)  is defined as the following mathematical relationship:

\begin{equation} \label{change_basis}
    \vec{z} = P^{-1} \vec{x}
\end{equation}

Applying this formula for the given $\vec{x} = \rcvec{2 & 1}^\intercal$ gives us $\vec{z} = \rcvec{z_1 & z_2} = \rcvec{0 & 1}^\intercal$

To verify this answer, (\ref{change_basis}) can be rearranged to return to the original basis, i.e., $\vec{z} \mapsto \vec{x} $:
\begin{equation*}
    \vec{x} = z_1\rcvec{1\\1} + z_2\rcvec{2\\1} = 0\rcvec{1\\1} + 1\rcvec{2\\1} = \rcvec{2\\1}
\end{equation*}

Hence, we numerically reconstructed $\vec{x}$ and verified that the change of basis computed the correct $\vec{z}$.

\subsubsection{Output 4}
A linear transformation $A : \mathbb{R}^n \rightarrow \mathbb{R}^m$ given by $\vec{y} = A(\vec{x}) = A\vec{x}$ can be expressed in the general coordinate frames, different from the standard bases, $P = \rcvec{\vec{x}_1 & \cdots & \vec{x}_n} \in \mathbb{R}^{n\times n}$ and $Q = \rcvec{\vec{y}_1 & \cdots & \vec{y}_m} \in \mathbb{R}^{m\times m}$, whose columns constitute bases for $\mathbb{R}^n$ and $\mathbb{R}^m$ respectively. Let this transformation between the two non-standard bases be given by,
\[
    \vec{w}=\hat{A}\vec{z},
\]
where, $\vec{x}=P\vec{z}$ and $\vec{y} = Q\vec{w}$. Then, we can write:
\begin{align*}
    \vec{w} = Q^{-1}\vec{y} &= \hat{A}P^{-1}\vec{x} = \hat{A}\vec{z}\\
    \implies \quad \vec{y} &= Q\hat{A}P^{-1}\vec{x} = A\vec{x}
\end{align*}

Therefore, we have that $\hat{A} = Q^{-1}AP=\texttt{Q\textbackslash (AP)}$ in \textsc{MATLAB} code, which means that vector ${\hat{A}}[:, j]$ is the coordinate of vector $AP[:, j]$ in basis $Q$ where $AP[:, j]$ is the mapping of the $j^{\text{th}}$ basis vector in $P$ into $\mathbb{R}^m$, i.e. $AP$ maps $P$-coordinates into $\mathbb{R}^m$ in the standard basis and $Q^{-1}$ further maps these standard coordinates into the non-standard basis. For the given matrices and bases, the matrix $\hat{A}$ was found to be:

\[
\hat{A} = 
\begin{bmatrix}
    0.5  &   2  & -1  & -2\\
    0.5  &   1  &  0  & 0\\
    0.5  &   0  &  3  & 4\\
    0.5  &   1  &  1  & 1\\
    0    &   0  &  2  & 2
\end{bmatrix}
\]

\subsubsection{Output 5}
Given a matrix $A$, to test for its injectivity, we simply need to see if the $\text{dim}(\text{Ker}(A)) = 0$. If it is nonzero, then the zero vector, $\vec{0}$, and at least one non-zero vector (in the nullspace) will have outputs equal to $\vec{0}$, implying that the linear transformation $A\vec{x}$ is not injective. This is only true if $A$ has full column rank. Similarly, as a test for surjectivity, we need the $\text{rank}(A)$ to equal the dimension of the co-domain, i.e., have full row rank. If $A$ is not a full rank matrix, then it would neither be injective nor surjective, and on the other hand, if $A$ is a full-rank square matrix, then it is a bijective map.

The rank($A$) was obtained using the \texttt{rank} function, and it was determined to be $3$. With the number of columns in $A$ being $4$,the rank-nullity theorem can be applied to determine the nullity($A$):
\begin{equation*}
    \text{nullity}(A) = 4 - \text{rank}(A) = \texttt{size(A,2) - rank(A)}= 4 - 3 = 1
\end{equation*}

This can be verified by directly computing the dimension of the nullspace of $A$:
\begin{equation*}
    \text{nullity}(A) = \text{dim(nullspace}(A)) = \texttt{size(null(A),2)} = 1
\end{equation*}

And by the programmatic implementation of the matrix map characterization criteria detailed above, it was determined that $A$ is neither an injective nor a surjective map.

\subsubsection{Output 6}
To test for solutions to $A\vec{x} = \vec{b}$, we can compare $\text{rank}(A) = 3$ with $\text{rank}(\rcvec{A & \vec{b}})$, where $\vec{b} \in \mathbb{R}^n$. Here, we are essentially checking if $\vec{b}$ is a linear combination of the column space of $A$. This logic was utilized in output 2 (section 1.1.2).

Note that if $\text{rank}(\rcvec{A & \vec{b}})$ is affected, i.e., if it increases from $\text{rank}(A)$, then there does not exist any $\vec{x}\in \mathbb{R}^n$ that solves the system of equations $A\vec{x} = \vec{b}$. If the rank of the augmented matrix is unaffected, then there is at least one solution. If the nullity($A$) is $0$, then the solution will be unique. If not, there will be an infinite number of solutions to the system. The table \ref{tab:solns} summarizes output 6 for provided $\vec{b}$.

\begin{table}[ht]
    \centering
    \begin{tabular}{|p{0.1\linewidth}|p{0.13\linewidth}|p{0.1\linewidth}|p{0.15\linewidth}|p{0.4\linewidth}|}
        \hline
        $\vec{b}$ & $\text{rank}(\rcvec{A & \vec{b}})$ & \#Solutions & Solution(s) & Comments \\ \hline & & & & \\
        $\rcvec{1\\0\\0\\0\\0}$ & 4 & 0 & N/A & The rank increases, implying no solution\\ & & & & \\ \hline & & & & \\
        $\rcvec{1\\1\\-2\\-2\\-2}$ & 3 & $\infty$ & $\rcvec{2\\-1\\0\\-1}, \rcvec{2.5\\-1.5\\0.5\\-1.5}$ & \vspace{-1cm}The nullity(A) is 1, so there are an infinite number of solutions. It is easy to generate another solution to the system by taking $x' = x + ky$, where x is the solution determined by using $\texttt{A\textbackslash b2}$, y is any vector in the nullspace, and $k \in \mathbb{R}$ \vspace{0.4cm} \\ \hline
    \end{tabular}
    \caption{Summary of output 6}
    \label{tab:solns}
\end{table}

\section{A-invariance and Representation Theorem}
A subspace $\mathcal{V}$ is $A$-invariant if and only if
\[
    A\mathcal{V} \subset \mathcal{V},
\]
i.e., for any vector $\vec{v} \in \mathcal{V}$ we have that $A\vec{v} \in \mathcal{V}$.

\subsection{Testing for A-invariance}
\subsubsection{Output 7}
For a given basis $V$ of the subspace $\mathcal{V}$, to numerically test for $A$-invariance, it is sufficient to check if $\text{rank}(V) = \text{rank}(\rcvec{AV & V})$. For the given basis, we have $\text{rank}(V) = 2 = \text{rank}(\rcvec{AV & V})$, and hence we can conclude that $\mathcal{V}$ is $A$-invariant.

To find the matrix $P = \rcvec{V & W}$ of Representation theorem, we need to compute $W$, which is the independent complement of $V$. For this, it is sufficient enough to find a basis for the kernel of $V^\intercal$:
\begin{equation*}
    W = \mathscr{Basis}_{\text{Ker}(\mathcal{V^\intercal})} = \texttt{null(V')} = \rcvec{0.5774 & 0.5774 & 0.5774}^\intercal
\end{equation*}

Now the matrix $P$ can be constructed, allowing us to compute $P^{-1}AP$:
\begin{equation*}
    P^{-1}AP = \texttt{P\textbackslash(A*P)} = \begin{bmatrix}
        2 & 0 & 0 \\
        0 & -1 & 2.3094 \\ 
        0 & 0 & 1
    \end{bmatrix}
\end{equation*}

It can be visually verified that $P^{-1}AP$ is block upper-triangular.

\section{Controllability and Kalman Decomposition}
\subsection{Controllability}
For a linear state-space system with matrices $(A,B)$, the controllability matrix is given by:
\[
    Q_c \coloneqq \rcvec{B & AB & \cdots & A^{n-1}B}
\]

For the given $A$ and $B$, the matrix $Q_c$ was computed by using the \texttt{ctrb} function in \textsc{MATLAB}:
\begin{equation*}
    \begin{bmatrix}
        1 & -2 & 10 & 0 & 20 & 40 \\
        0 & 0 & 0 & 0 & 0 & 0 \\
        1 & 2 & -6 & 8 & -44 & -8
    \end{bmatrix}
\end{equation*}

\subsection{Kalman Decomposition for Controllability}
\subsubsection{Output 8}
To determine $\hat{A}$ and $\hat{B}$, an $A$-invariant subspace $\mathcal{V}$ must be determined and then the same steps used in output 7 (section 2.1.1) can be utilized. We let $\mathcal{V}$ be the $\text{Im}(Q_c)$ which is an $A$-invariant subspace. By applying \texttt{orth} function to $Q_c$ to compute a basis for $\mathcal{V}$, we get the following:
\begin{equation*}
    V = \begin{bmatrix}
        -0.7071 & -0.7071 \\
        0 & 0 \\ 
        0.7071 & -0.7071
    \end{bmatrix}
\end{equation*}

The independent complement subspace $\mathcal{W}$ can be computed similar to how it was done in section 2, i.e., $\mathcal{W} = \text{null}(\mathcal{V}^\intercal)$, which gives us the basis:
\begin{equation*}
    W = 
        \begin{bmatrix}
            0\\1\\0
        \end{bmatrix}
\end{equation*}

Finally, we can set $P = \rcvec{V & W}$ to determine the Kalman decomposition, ($\hat{A}$, $\hat{B}$), for this system. $\hat{A}$ is computed as:
\begin{equation*}
    \hat{A} = P^{-1}AP = \texttt{P\textbackslash(A*P)} = 
    \begin{bmatrix}
        2 & 8 & 11.3137 \\
        -2 & 2 & 4.2426 \\ 
        0 & 0 & -6
    \end{bmatrix}
\end{equation*}

Similarly, $\hat{B}$ is computed as:
\begin{equation*}
    \hat{B} = P^{-1}B = \texttt{P\textbackslash B} = 
    \begin{bmatrix}
        0 & 2.8284 \\
        -1.4142 & 0 \\ 
        0 & 0 
    \end{bmatrix}
\end{equation*}

The system can then be expressed as:
\begin{equation*}
    \begin{bmatrix}
        \dot{z_1} \\
        \dot{z_2} \\
        \dot{z_3}
    \end{bmatrix}
    =
    \hat{A} \begin{bmatrix}
        z_1\\z_2\\z_3
    \end{bmatrix} +
    \hat{B} \begin{bmatrix}
        u_1 \\ u_2
    \end{bmatrix}
\end{equation*}

To split the system into its controllable and uncontrollable subsystems, we can use the structure of the $\hat{A}$ matrix:
\begin{equation*}
    \hat{A} = 
    \begin{bmatrix}
        A_{11} & A_{12}\\
        0 & A_{22}
    \end{bmatrix},
\end{equation*}
where $A_{11} \in \mathbb{R}^{k\times k}$, $A_{12} \in \mathbb{R}^{k \times n-k}$, $0 \in \mathbb{R}^{n-k \times k}$, $A_{22} \in \mathbb{R}^{n-k \times n-k}$, $k = \text{rank}(Q_c)$, i.e., the number of vectors in basis $V$, and $n$ is the number of rows in $Q_c$.

Since we have $k = 2$ and $n = 3$, the controllable subsystem is found to be:
\begin{equation*}
    \dot{z_1} = 2z_1 + 8z_2 + 11.3137z_3 + 2.8284u_2
\end{equation*}
\begin{equation*}
    \dot{z_2} = - 2z_1 + 2z_2 + 4.2426z_3 - 1.4142u_1
\end{equation*}

And the uncontrollable subsystem is found to be:
\begin{equation*}
    \dot{z_3} = -6z_3
\end{equation*}

Note, that while $z_3$ cannot be controlled directly or indirectly, it will asymptotically converge to $0$ and not blow up. 

\end{document}