\documentclass[10pt]{article}
\usepackage{../setup}
\vspace{-8ex}
\date{}

\graphicspath{ {./figs/} }

\begin{document}

\title{\textbf{\Large{\textsc{ECE410:} Linear Control Systems}} \\ \Large{Lab 2 Report: Numerical Linear Algebra and Controllability} \\ \textbf{\small{PRA102}}\vspace{-0.3cm}}
\author{Pranshu Malik, Varun Sampat \\ \footnotesize{1004138916}, \footnotesize{1003859602}\vspace{-3cm}}

\maketitle

\section{Numerical Linear Algebra}
\subsection{Basic operations on subspaces}
\subsubsection{Output 1}
Recall $ \mathcal{V} $ and $ \mathcal{W} $ were subspaces defined as:

\begin{equation*}
    \mathcal{V} = \text{span}
    \begin{Bmatrix}
        \rcvec{1\\-1\\0\\1},
        \rcvec{1\\1\\0\\0},
        \rcvec{3\\1\\0\\1}
    \end{Bmatrix}
    \quad
    \text{and}
    \quad
    \mathcal{W} = \text{span}
    \begin{Bmatrix}
        \rcvec{1\\0\\2\\1},
        \rcvec{1\\0\\-2\\0}
    \end{Bmatrix}
\end{equation*}

For these subspaces, \texttt{orth} returned:
\begin{equation*}
    \mathscr{Basis}_{\mathcal{V}} = \text{span}
    \begin{Bmatrix}
        \rcvec{-0.9089\\-0.2650\\0\\-0.3220},
        \rcvec{-0.0849\\0.8736\\0\\-0.4792}
    \end{Bmatrix}
    \quad
    \text{and}
    \quad
    \mathscr{Basis}_{\mathcal{W}} = \text{span}
    \begin{Bmatrix}
        \rcvec{-0.0399\\0\\-0.9645\\-0.2611},
        \rcvec{-0.8988\\0\\0.1488\\-0.4122}
    \end{Bmatrix}
\end{equation*}


The column space in $\mathcal{V}$ is not linearly independent. The rank$(\mathcal{V})$ is 2 < the dim($\mathcal{V}) = 3$. Note that the dim($\mathcal{V}$) would be the number of vectors in $\mathcal{V}$. This leads to the orthonormal basis of $\mathcal{V}$ containing only 2 vectors instead of 3. 

The column space in $\mathcal{W}$ was linearly independent. The orthnormal basis contains the same number of column vectors as $\mathcal{W}$ itself. One interesting point to note is that $\mathcal{W}$ contained $\rcvec{1 & 0 & -2 & 0}^\intercal$, but in its orthnormal basis, the 0 in the last row did not show up. This is because the \textsc{MATLAB} function \texttt{orth} returns vectors that are orthogonal (hence linearly independent) and normalized. \textcolor{red}{Show that spanning a hyperplane, and since one vector has a component along one of the axes, both basic vectors end up having a component since the first is not colinear}

\subsubsection{Output 2}
For the same $\mathcal{V}$ and $\mathcal{W}$ defined in section 1:
\begin{equation*}
    \text{Im}({\mathcal{V} + \mathcal{W}}) = \text{span}
    \begin{Bmatrix}
        \rcvec{-0.9004\\-0.2119\\-0.0950\\-0.3680},
        \rcvec{0.1634\\0.0904\\-0.9611\\-0.2038},
        \rcvec{0.0513\\-0.8871\\-0.1654\\0.4279}
    \end{Bmatrix}
\end{equation*}
\begin{center}
    and
\end{center}
\begin{equation*}
    \text{Im}({\mathcal{V} \cap \mathcal{W}}) = \text{span}
    \begin{Bmatrix}
        \rcvec{-0.8944\\0\\0\\-0.4472}
    \end{Bmatrix}
\end{equation*}

To ensure these computations are correct, we could test if ${\mathcal{V} \cap \mathcal{W}} \in \text{Im}({\mathcal{V} + \mathcal{W}})$. This is because any vector that is contained in both the vector spaces should also be contained in the vector space defined as the sum of the two vector spaces.

Essentially, we want to test if all vectors spanned by ${\mathcal{V} \cap \mathcal{W}}$ can be expressed as a linear combination of $\mathcal{V} + \mathcal{W}$:
\begin{equation*}
    \text{rank}(\mathcal{V} + \mathcal{W}) = 3
    \quad
    \text{and}
    \quad
    \text{rank}\left(\rcvec{\mathcal{V} + \mathcal{W} & \mathcal{V} \cap \mathcal{W}}\right) = 3
\end{equation*}

The addition of $\mathcal{V} \cap \mathcal{W}$ does not affect the $\text{Im}$ of $\mathcal{V} + \mathcal{W}$. Therefore, we can conclude that these computations are correct.

\subsection{Linear transformations and linear equations}
\subsubsection{Output 3}
Consider the basis $\{\vec{x}_1, \vec{x}_2\}$ of $\mathbb{R}^2$, where:
\begin{equation*}
    \vec{x}_1 = \rcvec{1\\1}
    \quad
    \text{and}
    \quad
    \vec{x}_2 = \rcvec{2\\1}
\end{equation*}

The change of basis (hence invertible) matrix $P$ will be defined as the concatenation of $\vec{x}_1$ and $\vec{x}_2$:
\begin{equation*}
    P =
        \begin{bmatrix}
            \vec{x}_1 & \vec{x}_2
        \end{bmatrix} = 
        \begin{bmatrix}
            1 & 2\\1 & 1
        \end{bmatrix}
\end{equation*}
Recall, the change of basis relationship ($\vec{x} \mapsto \vec{z} $)  is defined as the following mathematical relationship:

\begin{equation} \label{change_basis}
    \vec{z} = P^{-1} \cdot \vec{x}
\end{equation}

Applying this formula for the given $\vec{x} = \rcvec{2 & 1}^\intercal$ gives us $\vec{z} = \rcvec{z_1 & z_2} = \rcvec{0 & 1}^\intercal$

To verify this answer, (\ref{change_basis}) can be rearranged to return to the original basis, i.e., $\vec{z} \mapsto \vec{x} $:
\begin{equation*}
    \vec{x} = z_1\rcvec{1\\1} + z_2\rcvec{2\\1} = 0\rcvec{1\\1} + 1\rcvec{2\\1} = \rcvec{2\\1}
\end{equation*}

Hence, we numerically reconstructed $\vec{x}$ and verified that the change of basis computed the correct $\vec{z}$.

\subsubsection{Output 4}
A linear transformation $\vec{A} : \mathbb{R}^n \rightarrow \mathbb{R}^m$ given by $\vec{y} = \vec{A}(\vec{x}) = A\vec{x}$ can be expressed in the general coordinate frames, different from the standard bases, $P = \rcvec{\vec{x}_1 & \cdots & \vec{x}_n} \in \mathbb{R}^{n\times n}$ and $Q = \rcvec{\vec{y}_1 & \cdots & \vec{y}_m} \in \mathbb{R}^{m\times m}$, whose columns constitute bases for $\mathbb{R}^n$ and $\mathbb{R}^m$ respectively. Let this transformation between the two non-standard bases be given by,
\[
    \vec{w}=\hat{A}\vec{z},
\]
where, $\vec{x}=P\vec{z}$ and $\vec{y} = Q\vec{w}$. Then, we can write:
\begin{align*}
    \vec{w} = Q^{-1}\vec{y} &= \hat{A}P^{-1}\vec{x} = \hat{A}\vec{z}\\
    \implies \quad \vec{y} &= Q\hat{A}P^{-1}\vec{x} = A\vec{x}
\end{align*}

Therefore, we have that $\hat{A} = Q^{-1}AP=\texttt{Q\textbackslash (AP)}$ in \textsc{MATLAB} code, which means that vector ${\hat{A}}[:, j]$ is the coordinate of vector $AP[:, j]$ in basis $Q$ where $AP[:, j]$ is the mapping of the $j^{\text{th}}$ basis vector in $P$ into $\mathbb{R}^m$, i.e. $AP$ maps $P$-coordinates into $\mathbb{R}^m$ in the standard basis and $Q^{-1}$ further maps these standard coordinates into the non-standard basis.

\subsubsection{Output 5}
Given a matrix $A$, to test for its injectivity, we simply need to see if the $\text{dim}(\text{Ker}A) = 0$. If it is nonzero, then the zero vector and at least one non-zero vector (in the nullspace) have outputs equal $\vec{0}$, implying that the linear transformation $A\vec{x}$ is not injective. This is only true if $A$ has full column rank. Similarly, as a test for surjectivity, we need the $\text{rank}(A)$ to equal the dimension of the co-domain, i.e., have full row rank. If $A$ is not a full rank matrix, then it would neither be injective nor surjective, and on the other hand, if $A$ is a full-rank square matrix, then it is a bijective map.

\textcolor{red}{Add MATLAB results}

\subsubsection{Output 6}
\textcolor{red}{Add some intro theory and add MATLAB results}
\section{A-invariance and Representation Theorem}
A subspace $\mathcal{V}$ is $A$-invariant if and only if
\[
    A\mathcal{V} \subset \mathcal{V},
\]
i.e., for any vector $\vec{v} \in \mathcal{V}$ we have that $A\vec{v} \in \mathcal{V}$.
\subsection{Testing for A-invariance}
We can test this by...
\textcolor{red}{Add some intro theory and add MATLAB results}
\subsubsection{Output 7}
\textcolor{red}{Add some intro theory and add MATLAB results}

\section{Controllability and Kalman Decomposition}
What is controllability and Kalman: 2 sentences each with all terms needed.
\textcolor{red}{Add some intro theory and add MATLAB results}
\subsection{Controllability}
For a linear state-space system with matrices $(\vec{A},\vec{B})$
\[
    Q_c \coloneqq \rcvec{B & AB & \cdots & A^{n-1}B}
\]
\textcolor{red}{Add some intro theory and add MATLAB results}
\subsection{Kalman Decomposition for Controllability}
\subsubsection{Output 8}
\textcolor{red}{Add some intro theory and add MATLAB results}
\end{document}